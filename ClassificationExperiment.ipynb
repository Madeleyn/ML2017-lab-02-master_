{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, model_selection\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "#load data\n",
    "def get_data(path, n_features=None):\n",
    "    if n_features == None:\n",
    "        X, y = datasets.load_svmlight_file(path)\n",
    "    else:\n",
    "        X, y = datasets.load_svmlight_file(path, n_features=n_features)\n",
    "    #append one column\n",
    "    X = np.hstack([X.toarray(), np.ones((X.shape[0], 1))])\n",
    "    y = np.array(y).reshape(X.shape[0],1)\n",
    "    y[y==-1] = 0 #if y == -1, then y = 0\n",
    "    return X, y\n",
    "\n",
    "#loss_function\n",
    "#def compute_loss(X, Y, theta, C=1):\n",
    " #   epsilon_loss = 1 - Y * X.dot(theta)\n",
    "  #  epsilon_loss[epsilon_loss<0] = 0\n",
    "   # loss = 0.5 * np.dot(theta.transpose(), theta).sum() + C*epsilon_loss.sum()\n",
    "    #return loss/X.shape[0]\n",
    "def compute_loss(X, y, theta, C):\n",
    "    loss = (np.linalg.norm(theta,2)**2)/2 + C * np.mean(np.maximum(1 - y * X.dot(theta.T), 0))\n",
    "    return loss\n",
    "    \n",
    "#gradient value\n",
    "def gradient(X_train, Y_train, theta, C):\n",
    "    grad = C*grad/X_train.shape[0] + theta\n",
    "    return grad\n",
    "\n",
    "#get part of sample\n",
    "def get_part(X, y ,min_part):\n",
    "    i = np.random.randint(0, X.shape[0], size=min_part, dtype=int) #generate random int from 0 to 32561\n",
    "    return X[i,:], y[i]    \n",
    "\n",
    "#show function\n",
    "def show(train_loss, test_loss):\n",
    "    plt.plot(train_loss,'red',label='Train')\n",
    "    plt.plot(test_loss,'black',label='test')\n",
    "    plt.xlabel('round number')\n",
    "    plt.ylabel('Loss value')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LogisticClassification(X_train, y_train, theta, C,\n",
    "                           learning_rate,\n",
    "                           optimizer=None, \n",
    "                           optimizer_params=None):\n",
    "    if optimizer == None:\n",
    "        gred = gradient(X_train, y_train, theta)\n",
    "        theta = theta - learning_rate*gred\n",
    "    elif optimizer == \"NAG\":\n",
    "        #initialize v and Gamma\n",
    "        v = np.zeros(theta.shape)\n",
    "        Gamma = 0.9\n",
    "            \n",
    "        grad = gradient(X_train, y_train, theta- Gamma*v)\n",
    "        v = Gamma*v + learning_rate*grad\n",
    "        theta = theta - v\n",
    "\n",
    "        #optimizer_params['Velocity'] = v\n",
    "    elif optimizer == \"RMSProp\":\n",
    "        G = np.zeros(theta.shape)\n",
    "        Gamma = 0.9\n",
    "        Epsilon = 1e-7\n",
    "\n",
    "        grad = gradient(X_train, y_train, theta)\n",
    "        G = Gamma*G + (1-Gamma)*(grad**2)            \n",
    "        theta = theta - learning_rate*grad/(np.sqrt(G+Epsilon))\n",
    "            \n",
    "        #optimizer_params['Velocity'] = G\n",
    "    elif optimizer == \"Adadelta\":\n",
    "        G = np.zeros(theta.shape)\n",
    "        Delta = np.zeros(theta.shape)\n",
    "        Gamma = 0.9\n",
    "        Epsilon = 1e-7\n",
    "\n",
    "        grad = gradient(X_train, y_train, theta)\n",
    "        G = Gamma*G + (1-Gamma)*(grad**2)\n",
    "        DeltaTheta = -((np.sqrt(Delta+Epsilon))/(np.sqrt(G+Epsilon)))*grad\n",
    "        \n",
    "        theta = theta + DeltaTheta\n",
    "        Delta = Delta*Gamma + (1-Gamma)*(DeltaTheta**2)\n",
    "            \n",
    "        #optimizer_params['Delta'] = Delta\n",
    "        #optimizer_params['G'] = G\n",
    "    elif optimizer == \"Adam\":\n",
    "        m = np.zeros(theta.shape)\n",
    "        G = np.zeros(theta.shape)\n",
    "        Alpha = np.zeros(theta.shape)\n",
    "        Beta = 0.9\n",
    "        Gamma = 0.999\n",
    "        Epsilon = 1e-8\n",
    "            \n",
    "        grad = gradient(X_train, y_train, theta)\n",
    "        m = Beta*m + (1-Beta)*grad\n",
    "        G = Gamma*G + (1-Gamma)*(grad**2)\n",
    "        Alpha = learning_rate * (np.sqrt(1-Gamma))/(1-Beta)\n",
    "        theta = theta - Alpha*m/(np.sqrt(G + Epsilon))\n",
    "\n",
    "        #optimizer_params['m'] = m\n",
    "        #optimizer_params['G'] = G\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main(learning_rate=0.0003,\n",
    "         C=5,\n",
    "         threshold=0,\n",
    "         min_part=64, \n",
    "         num_round=2000,     \n",
    "         optimizer=None):\n",
    "    trian_path = 'C:\\Users\\USER\\PythonProjects\\ML2017-lab-02-master\\DATA\\a9a.txt'\n",
    "    test_path = 'C:\\Users\\USER\\PythonProjects\\ML2017-lab-02-master\\DATA\\a9a.txt'\n",
    "    X_train, y_train = get_data(trian_path)\n",
    "    X_test, y_test = get_data(test_path, X_train.shape[1]-1)\n",
    "    theta = np.random.random((X_train.shape[1], 1)) #random initialize\n",
    "    train_loss_history = []\n",
    "    test_loss_history = []\n",
    "    for item in range(num_round):\n",
    "        X_train_part, y_train_part = get_part(X_train, y_train, min_part)\n",
    "        theta = LogisticClassification(X_train_part, y_train_part, theta, learning_rate=learning_rate, optimizer=optimizer)\n",
    "        \n",
    "        loss = compute_loss(X_train_part, y_train_part, theta)\n",
    "        train_loss_history.append(loss)\n",
    "        \n",
    "        X_test_part, y_test_part = get_part(X_test, y_test, min_part)\n",
    "        loss = compute_loss(X_test_part, y_test_part, theta)\n",
    "        test_loss_history.append(loss)\n",
    "    \n",
    "    \n",
    "    if optimizer != None:\n",
    "        print(optimizer)\n",
    "    if optimizer == None:\n",
    "        print(\"without optimizer\")\n",
    "    show(train_loss_history, test_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "main(learning_rate=0.003,\n",
    "     C=5,\n",
    "     threshold=0,\n",
    "     min_part=128,\n",
    "     num_round=2000)\n",
    "#NAG\n",
    "main(learning_rate=0.003,\n",
    "     C=5,\n",
    "     threshold=0,\n",
    "     min_part=128,\n",
    "     num_round=2000,\n",
    "     optimizer='NAG')\n",
    "#RMSProp\n",
    "main(learning_rate=0.003,\n",
    "     C=5,\n",
    "     threshold=0.5,\n",
    "     min_part=1024,\n",
    "     num_round=2000,\n",
    "     optimizer='RMSProp')\n",
    "#Adadelta\n",
    "main(learning_rate=0.001,\n",
    "     C=5,\n",
    "     threshold=0.5,\n",
    "     min_part=1024,\n",
    "     num_round=2000,\n",
    "     optimizer='Adadelta')\n",
    "#Adam\n",
    "main(learning_rate=0.001,\n",
    "     C=5,\n",
    "     threshold=0.5,\n",
    "     min_part=1024,\n",
    "     num_round=2000,\n",
    "     optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
